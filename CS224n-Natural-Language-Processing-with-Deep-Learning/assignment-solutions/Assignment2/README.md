# Assignment2è®²è§£

[Assignment2](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a2.zip)(ç‚¹å‡»ä¸‹è½½) çš„ä»»åŠ¡ä¾æ—§æ˜¯è¯å‘é‡ï¼Œä½†åœ¨è¿™é‡Œå°†ä»¥æ›´æ‰‹åŠ¨çš„æ–¹å¼å»æ„å»ºword2vecï¼Œå¸®åŠ©å¤§å®¶ç†Ÿæ‚‰pythonå’Œpytorchï¼Œæ–¹ä¾¿ä¹‹åè¿›è¡Œæƒ³æ³•å®ç°ã€è®ºæ–‡å¤ç°ç­‰å·¥ä½œã€‚Assignment2çš„ä½œä¸šæ–‡æ¡£å¯ä»¥åœ¨[è¿™é‡Œ](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a2.pdf)ä¸‹è½½åˆ°ã€‚

## è§£ç­”ï¼šç†è§£è¯å‘é‡ï¼ˆ23åˆ†ï¼‰

æˆ‘ä»¬å…ˆå¿«é€Ÿå›é¡¾ä¸€ä¸‹word2vecç®—æ³•ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯â€œä¸€ä¸ªè¯çš„å«ä¹‰å¯ä»¥ç”±å®ƒå‘¨å›´çš„è¯æ¥è¡¨ç¤ºâ€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªä¸­å¿ƒè¯ï¼ˆcenter wordï¼‰ *c*ï¼Œå’Œè¿™ä¸ªè¯ *c* å‘¨å›´ä¸Šä¸‹æ–‡æ„æˆçš„çª—å£ï¼Œè¿™ä¸ªçª—å£å†…çš„é™¤äº† *c* ä¹‹å¤–çš„è¯å«åšå¤–å›´è¯ï¼ˆoutside wordsï¼‰ã€‚æ¯”å¦‚ä¸‹å›¾ä¸­ï¼Œä¸­å¿ƒè¯æ˜¯â€œbankingâ€ï¼Œçª—å£å¤§å°ä¸º2ï¼Œæ‰€ä»¥ä¸Šä¸‹æ–‡çª—å£æ˜¯ï¼šâ€œturningâ€ã€â€intoâ€œã€â€crisesâ€œå’Œâ€asâ€œã€‚

<img src="http://ww1.sinaimg.cn/large/0060yMmAly1gsz2qlc163j30kn06xwh0.jpg" referrerpolicy="no-referrer" />

Skip-gramæ¨¡å‹ï¼ˆword2vecæ¯”è¾ƒå¸¸ç”¨çš„ä¸€ç§å®ç°ï¼Œå¦ä¸€ç§æ˜¯cbowï¼‰ç›®çš„æ˜¯å­¦ä¹ æ¦‚ç‡åˆ†å¸ƒ $ğ‘ƒ(ğ‘‚|ğ¶)$ã€‚è¿™æ ·ä¸€æ¥ï¼Œå°±èƒ½è®¡ç®—ç»™å®šçš„ä¸€ä¸ªè¯ $o$ å’Œè¯ $c$ çš„æ¦‚ç‡ $ğ‘ƒ(ğ‘‚=ğ‘œ|ğ¶=ğ‘)$ï¼ˆå³ï¼Œåœ¨å·²çŸ¥è¯ $c$ å‡ºç°çš„æƒ…å†µä¸‹ï¼Œè¯ $o$ å‡ºç°çš„æ¦‚ç‡ï¼‰ï¼Œ å…¶ä¸­$c$ æ˜¯ä¸­å¿ƒè¯ï¼Œ$o$ æ˜¯çª—å£ä¸­éä¸­å¿ƒçš„å¤–å›´è¯ã€‚

åœ¨word2vecä¸­ï¼Œè¿™ä¸ªæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæ˜¯é€šè¿‡è®¡ç®—å‘é‡ç‚¹ç§¯ï¼ˆdot-productsï¼‰ï¼Œå†åº”ç”¨naive-softmaxå‡½æ•°å¾—åˆ°çš„ï¼š
$$
P(O=o \mid C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w \in \operatorname{Vocab}} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}
$$


è¿™é‡Œï¼Œ$ğ‘¢_o$å‘é‡ä»£è¡¨å¤–å›´è¯ï¼Œ$v_c$å‘é‡ä»£è¡¨ä¸­å¿ƒè¯ã€‚ä¸ºäº†åŒ…å«è¿™äº›å‘é‡ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªçŸ©é˜µ $ğ‘ˆ$ å’Œ $ğ‘‰$ ã€‚ $ğ‘ˆ$ çš„åˆ—æ˜¯å¤–å›´è¯ï¼Œ $V$ çš„åˆ—æ˜¯ä¸­å¿ƒè¯ï¼Œè¿™ä¸¤çŸ©é˜µéƒ½æœ‰æ‰€æœ‰è¯$w \in Vocabulary$çš„è¡¨ç¤º ã€‚

å¯¹äºè¯ $c$ å’Œè¯ $o$ï¼ŒæŸå¤±å‡½æ•°ä¸ºå¯¹æ•°å‡ ç‡ï¼š
$$
\boldsymbol{J}_{naive-softmax}(v_c, o, \boldsymbol{U}) = -log P(O=o \mid C=c)
$$


å¯ä»¥ä»äº¤å‰ç†µçš„è§’åº¦çœ‹è¿™ä¸ªæŸå¤±å‡½æ•°ã€‚çœŸå®å€¼ä¸º $y$ ï¼Œæ˜¯ä¸€ä¸ªç‹¬çƒ­å‘é‡ï¼Œé¢„æµ‹å€¼ $\hat{y}$ è®¡ç®—å¾—åˆ°ã€‚å…·ä½“æ¥è¯´ï¼Œ $y$ å¦‚æœæ˜¯ç¬¬kä¸ªå•è¯ï¼Œé‚£ä¹ˆå®ƒçš„ç¬¬kç»´ä¸º1ï¼Œå…¶ä½™ç»´éƒ½æ˜¯0ï¼Œè€Œ $\hat{y}$ çš„ç¬¬kç»´è¡¨ç¤ºè¿™æ˜¯ç¬¬kä¸ªè¯çš„æ¦‚ç‡å¤§å°ã€‚



### é—®é¢˜(a) (3åˆ†)

è¯æ˜å…¬å¼(2)ç»™å‡ºçš„naive-softmaxçš„æŸå¤±å‡½æ•°ï¼Œå’Œ $y$ ä¸ $\hat{y}$ çš„äº¤å‰ç†µæŸå¤±å‡½æ•°æ˜¯ä¸€æ ·çš„ï¼Œå‡å¦‚ä¸‹æ‰€ç¤ºï¼ˆç­”æ¡ˆæ§åˆ¶åœ¨ä¸€è¡Œï¼‰

$$-\sum_{w \in V o c a b} y_{w} \log \left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)$$

**ç­”ï¼š**å› ä¸ºé™¤äº† $o$ ä¹‹å¤–çš„è¯éƒ½ä¸åœ¨çª—å£å†…ï¼Œæ‰€ä»¥åªæœ‰è¯ $o$ å¯¹æŸå¤±å‡½æ•°æœ‰è´¡çŒ®

### é—®é¢˜(b) (5åˆ†)

è®¡ç®—æŸå¤±å‡½æ•° $\boldsymbol{J}_{naive-softmax}(v_c, o, \boldsymbol{U})$ å¯¹ä¸­å¿ƒè¯ $v_c$ çš„åå¯¼æ•°ï¼Œç”¨  $y$ ï¼Œ$\hat{y}$å’Œ $ğ‘ˆ$ æ¥è¡¨ç¤ºã€‚

**ç­”ï¼š**
$$
\begin{aligned}
J_{\text {naive-softmax} }\left(\boldsymbol{v}_{c}, o, \boldsymbol{U}\right)
&=-\log P(O=o | C=c) \\
&= -\log \frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}
{\sum_{w \in \operatorname{Vocab} } \exp \left(\boldsymbol{u}_{\boldsymbol{w} }^{\top} \boldsymbol{v}_{c}\right)} \\
&= - {u}_{o}^{\top}{v}_{c} + \log \sum_{w \in \operatorname{Vocab} } \exp \left(\boldsymbol{u}_{\boldsymbol{w} }^{\top} \boldsymbol{v}_{c}\right)
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial J_{\text {naive-softmax} }\left(\boldsymbol{v}_{c}, o, \boldsymbol{U}\right)}{\partial  v_c}
&= -u_o + \sum_{o \in \operatorname{Vocab} }\frac{\exp(u_o^\top v_c)}{\sum_{w \in \operatorname{Vocab} } \exp \left(\boldsymbol{u}_{\boldsymbol{w} }^{\top} \boldsymbol{v}_{c}\right)} 
\frac{\partial (u_o^\top v_c)}{\partial v_c}\\
&=-u_o + \sum_{o \in \operatorname{Vocab} } P(O=o | C=c)  u_o \\
&=- U y + U \hat y \\
&= U(\hat y - y)
\end{aligned}
$$


### é—®é¢˜(c) (5åˆ†)

è®¡ç®—æŸå¤±å‡½æ•° $\boldsymbol{J}_{naive-softmax}(v_c, o, \boldsymbol{U})$  å¯¹ä¸Šä¸‹æ–‡çª—å£å†…çš„è¯ $w$ çš„åå¯¼æ•°ï¼Œè€ƒè™‘ä¸¤ç§æƒ…å†µï¼Œå³ $w$ æ˜¯å¤–å›´è¯ $o$ï¼Œå’Œ $w$ ä¸æ˜¯ $o$ï¼Œç”¨ $y$ ï¼Œ$\hat{y}$å’Œ $v_c$ æ¥è¡¨ç¤ºã€‚

**ç­”ï¼š**
$$
\begin{aligned}
\frac{\partial J_{\text {naive-softmax} }\left(\boldsymbol{v}_{c}, o, \boldsymbol{U}\right)}{\partial  u_w}
&= -v_c 1_{\lbrace w=o \rbrace } + \frac{\exp(u_w^\top v_c)}{\sum_{w \in \operatorname{Vocab} } \exp \left(\boldsymbol{u}_{\boldsymbol{w} }^{\top} \boldsymbol{v}_{c}\right)} 
\frac{\partial (u_w^\top v_c)}{\partial u_w}\\
&=-v_c  1_{\lbrace w=o \rbrace } +  P(O=w | C=c)  v_c \\
&=v_c( \hat y_w -  y_w)
\end{aligned}
$$


### é—®é¢˜(d) (3åˆ†)

sigmoidå‡½æ•°å¦‚å…¬å¼æ‰€ç¤º
$$
\sigma(\boldsymbol{x})=\frac{1}{1+e^{-\boldsymbol{x}}}=\frac{e^{\boldsymbol{x}}}{e^{\boldsymbol{x}}+1}
$$
è¯·è®¡ç®—å‡ºå®ƒå¯¹äº $x$ çš„å¯¼æ•°ï¼Œ $x$ æ˜¯ä¸€ä¸ªå‘é‡

**ç­”ï¼š**

è®¡ç®—é›…å…‹æ¯”çŸ©é˜µå¯å¾—
$$
\begin{aligned}
\frac{\partial \sigma(x_i )}{\partial x_j }
&= \sigma (x_i) (1 -\sigma(x_i)) 1_{\lbrace i=j\rbrace  }
\end{aligned}
$$
æ‰€ä»¥æœ‰
$$
\frac{\partial \sigma(x)}{\partial  x}
=\text{diag}(\sigma(x) (1- \sigma(x)))
$$


### é—®é¢˜(e) (4åˆ†)

ç°åœ¨æˆ‘ä»¬è€ƒè™‘è´Ÿé‡‡æ ·çš„æŸå¤±å‡½æ•°ã€‚å‡è®¾æœ‰Kä¸ªè´Ÿæ ·æœ¬ï¼Œè¡¨ç¤ºä¸º$w_1, w_2, â€¦, w_K$ï¼Œå®ƒä»¬å¯¹åº”çš„å‘é‡ä¸º $u_1, u_2, â€¦, u_K$ï¼Œå¤–å›´è¯ $o \not\in {w_1, w_2, â€¦, w_K}$ï¼Œåˆ™å¤–å›´è¯ $o$ åœ¨ä¸­å¿ƒè¯æ˜¯ $c$ æ—¶äº§ç”Ÿçš„æŸå¤±å‡½æ•°å¦‚å…¬å¼æ‰€ç¤ºã€‚
$$
\boldsymbol{J}_{\text {neg-sample }}\left(\boldsymbol{v}_{c}, o, \boldsymbol{U}\right)=-\log \left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right)
$$
æ ¹æ®è¯¥æŸå¤±å‡½æ•°ï¼Œé‡æ–°è®¡ç®—é—®é¢˜(b)ã€é—®é¢˜(c)çš„åå¯¼æ•°ï¼Œç”¨ $\boldsymbol{u}_o$ï¼Œ$\boldsymbol{v}_c$ï¼Œ$\boldsymbol{u}_k$ æ¥è¡¨ç¤ºã€‚

å®Œæˆè®¡ç®—åï¼Œç®€è¦è§£é‡Šä¸ºä»€ä¹ˆè¿™ä¸ªæŸå¤±å‡½æ•°æ¯”naive-softmaxæ•ˆç‡æ›´é«˜ã€‚

æ³¨æ„ï¼šä½ å¯ä»¥ç”¨é—®é¢˜(d)çš„ç­”æ¡ˆæ¥å¸®åŠ©ä½ è®¡ç®—å¯¼æ•°

**ç­”ï¼š**
$$
\begin{aligned}
\frac{\partial J_{\text {neg-sample} }\left(v_{c}, o, U\right)}{\partial  v_c}
&=-\frac{\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\left(1- \sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\right)}{\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}u _o 
-\sum_{k=1}^K 
\frac{\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\left(1- \sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right)}
{\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)}(-u_k)\\
&= -\left(1- \sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\right)u_o
+ \sum_{k=1}^K  \left(1- \sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right)u_k\\
\frac{\partial J_{\text {neg-sample} }\left(v_{c}, o, U\right)}{\partial  u_o}
&=-\frac{\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\left(1- \sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\right)}{\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}v _c 
\\
&= -\left(1- \sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\right)v_c \\
\frac{\partial J_{\text {neg-sample} }\left(v_{c}, o, U\right)}{\partial  u_k}
&=
-
\frac{\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\left(1- \sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right)}
{\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)}(-v_c)\\
&= \left(1- \sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right)v_c
\end{aligned}
$$


- åŸå§‹çš„æŸå¤±å‡½æ•°ä¸­éœ€è¦æ±‚æŒ‡æ•°å’Œï¼Œå¾ˆå®¹æ˜“æº¢å‡ºï¼Œè´Ÿé‡‡æ ·çš„æŸå¤±å‡½æ•°èƒ½å¾ˆå¥½åœ°è§„é¿è¿™ä¸ªé—®é¢˜ã€‚

- è¯åº“ä» ğ‘‰ğ‘œğ‘ğ‘ğ‘ å˜æˆäº†K+1ä¸ªè¯
- åœ¨æ±‚å†…å±‚å¯¼æ•°çš„æ—¶å€™ç”¨äº†sigmoidå‡½æ•°

### é—®é¢˜(f) (3åˆ†)

å‡è®¾ä¸­å¿ƒè¯æ˜¯ $c = w_t$ï¼Œä¸Šä¸‹æ–‡çª—å£æ˜¯$[w_{t-m}, â€¦, w_{t-1}, w_t, w_{t+1}, â€¦, w_{t+m}]$ï¼Œ$m$ æ˜¯çª—å£å¤§å°ï¼Œå›é¡¾skip-gramçš„word2vecå®ç°ï¼Œåœ¨è¯¥çª—å£ä¸‹çš„æ€»æŸå¤±å‡½æ•°æ˜¯ï¼š
$$
\boldsymbol{J}_{\text {skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j \neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)
$$
è¿™é‡Œï¼Œ$\boldsymbol{J}(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U})$æ˜¯å¤–å›´è¯æ˜¯å¤–å›´è¯$w_{t+j}$åœ¨ä¸­å¿ƒè¯åœ¨ä¸­å¿ƒè¯$c=w_t$ ä¸‹äº§ç”Ÿçš„æŸå¤±ï¼ŒæŸå¤±å‡½æ•°å¯ä»¥æ˜¯naive-softmaxæˆ–è€…æ˜¯neg-sampleï¼ˆè´Ÿé‡‡æ ·ï¼‰ï¼Œè¿™å–å†³äºå…·ä½“å®ç°ã€‚

è®¡ç®—ï¼š

(i) æŸå¤±å‡½æ•°å¯¹ $U$ çš„åå¯¼æ•°

(ii) æŸå¤±å‡½æ•°å¯¹ $\boldsymbol{v}_c$ çš„åå¯¼æ•°

(iii) æŸå¤±å‡½æ•°å¯¹ $\boldsymbol{v}_w$ çš„åå¯¼æ•°

**ç­”ï¼š**
$$
\begin{aligned}
\partial \boldsymbol{J}_{\text {skip-gram } }\left(\boldsymbol{v}_{c}, w_{t-m}, \dots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U}
&=\sum_{-m \leq j \leq m \atop j \neq 0} \partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U} \\

\partial \boldsymbol{J}_{\text {skip-gram } }\left(\boldsymbol{v}_{c}, w_{t-m}, \dots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{v_c}
&=\sum_{-m \leq j \leq m \atop j \neq 0} \partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial  \boldsymbol{v_c} \\
\partial \boldsymbol{J}_{\text {skip-gram } }\left(\boldsymbol{v}_{c}, w_{t-m}, \dots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{v_w}
&=\sum_{-m \leq j \leq m \atop j \neq 0} \partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial  \boldsymbol{v_w} \\
\end{aligned}
$$


## ä»£ç ï¼šå®ç°word2vecï¼ˆ20åˆ†ï¼‰

ç‚¹å‡» [æ­¤å¤„](http://web.stanford.edu/class/cs224n/assignments/a2.zip) ä¸‹è½½ä»£ç ï¼Œpythonç‰ˆæœ¬ >= 3.5ï¼Œéœ€è¦å®‰è£…numpyï¼Œä½ åˆ©ç”¨condaæ¥é…ç½®ç¯å¢ƒï¼š

```shell
conda env create -f env.yml
conda activate a2
```

å†™å®Œä»£ç åï¼Œè¿è¡Œï¼š

```shell
conda deactivate
```

### é—®é¢˜(a) (12åˆ†)

é¦–å…ˆï¼Œå®ç° word2vec.py é‡Œçš„ sigmoidå‡½æ•°ï¼Œè¦æ”¯æŒå‘é‡è¾“å…¥ã€‚æ¥ç€å®ç°åŒä¸€ä¸ªæ–‡ä»¶é‡Œçš„ softmax ã€è´Ÿé‡‡æ ·æŸå¤±å’Œå¯¼æ•°ã€‚ç„¶åå®ç°skip-gramçš„æŸå¤±å‡½æ•°å’Œå¯¼æ•°ã€‚å…¨éƒ¨åšå®Œä¹‹åï¼Œè¿è¡Œpython word2vec.pyæ¥æ£€æŸ¥æ˜¯å¦æ­£ç¡®ã€‚

**ç­”ï¼š**

#### sigmoid

numpyå…·å¤‡å¹¿æ’­ç‰¹æ€§ï¼Œæœ€ç»ˆå¾—åˆ°å‘é‡è¾“å‡º

```python
def sigmoid(x):
    """
    Compute the sigmoid function for the input here.
    Arguments:
    x -- A scalar or numpy array.
    Return:
    s -- sigmoid(x)
    """

    ### YOUR CODE HERE
    s = 1 / (1 + np.exp(-x))

    ### END YOUR CODE

    return s
```

#### naiveSoftmaxLossAndGradient

è¿™ä¸ªæ¨¡å‹å…¶å®å°±æ˜¯ä¸€ä¸ªä¸‰å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œåªéœ€è¦æ³¨æ„ç»´åº¦å³å¯ï¼Œæ³¨é‡Šé‡Œå·²ç»æ ‡è®°å‡ºäº†ç»´åº¦ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå•è¯è¡¨ç¤ºæ˜¯åœ¨è¡Œï¼Œè€Œä¸æ˜¯åˆ—ã€‚

```python
### YOUR CODE HERE

### Please use the provided softmax function (imported earlier in this file)
### This numerically stable implementation helps you avoid issues pertaining
### to integer overflow. 
'''
    centerWordVec: 1 * d
    outsideVectors: n * d
    '''
#1 * n
vec = centerWordVec.dot(outsideVectors.T)
#1 * n
prob = softmax(vec)
loss = -np.log(prob[outsideWordIdx])
#1 * d
gradCenterVec = -outsideVectors[outsideWordIdx] + prob.dot(outsideVectors)
#n * d
gradOutsideVecs = prob.reshape(-1, 1).dot(centerWordVec.reshape(1, -1))
#n * d
gradOutsideVecs[outsideWordIdx] -= centerWordVec
### END YOUR CODE
```

#### negSamplingLossAndGradient

ä¸native-softmaxä¸åŒçš„æ˜¯ï¼š

- åªé€‰å–Kä¸ªéå¤–å›´è¯ä½œä¸ºè´Ÿæ ·æœ¬ï¼ŒåŠ ä¸Š1ä¸ªæ­£ç¡®çš„å¤–å›´è¯ï¼Œå…±K+1ä¸ªè¾“å‡º
- æœ€åä¸€å±‚ä½¿ç”¨sigmoidè¾“å‡ºï¼Œè€Œä¸æ˜¯softmax

æ³¨æ„ï¼Œåå‘ä¼ æ’­å¾—åˆ°çš„æ˜¯è¿™K+1ä¸ªè¯çš„æ¢¯åº¦ï¼Œæ‰€ä»¥éœ€è¦æŒ¨ä¸ªæ›´æ–°åˆ° *æ¢¯åº¦çŸ©é˜µ* ä¸­å»

```python
### Please use your implementation of sigmoid in here.

# indices might have same index
# extract W
W = np.zeros((len(indices), outsideVectors.shape[1]))
for i in range(len(indices)):
    W[i] = outsideVectors[indices[i]]

# forward
a = centerWordVec
a = a.reshape((a.shape[0], 1))

z = np.dot(W, a) # (K+1, 1)
preds = sigmoid(z)

# backprop
y = np.zeros((preds.shape[0], 1))
y[0] = 1 # index 0 is target

loss = -(y*np.log(preds) + (1 - y)*np.log(1 - preds)).sum()

delta = preds - y
gradCenterVec = np.dot(W.T, delta) # (V, 1)
gradW = np.dot(delta, a.T) # (K+1, V)
gradCenterVec = gradCenterVec.flatten()

# apply gradW into gradOutsideVecs
gradOutsideVecs = np.zeros_like(outsideVectors)
for i in range(len(indices)):
    oi = indices[i]
    gradOutsideVecs[oi] += gradW[i]
```

#### skipgram

éå†æ‰€æœ‰çš„å¤–å›´è¯ï¼Œæ±‚å’ŒæŸå¤±å‡½æ•°

```python
ci = word2Ind[currentCenterWord]
vc = centerWordVectors[ci]

for o in outsideWords:
    oi = word2Ind[o]
    loss_, gradVc, gradUo = word2vecLossAndGradient(vc, oi, outsideVectors, dataset)
    gradCenterVecs[ci] += gradVc
    gradOutsideVectors += gradUo
    loss += loss_
```

### é—®é¢˜(b) (4åˆ†)

å®Œæˆsgd.pyæ–‡ä»¶çš„SGDä¼˜åŒ–å™¨ï¼Œè¿è¡Œpython sgd.pyæ¥æ£€æŸ¥æ˜¯å¦æ­£ç¡®ã€‚

**ç­”**ï¼š

#### sgd

è°ƒç”¨å‡½æ•°å¾—åˆ°æŸå¤±å€¼å’Œæ¢¯åº¦ï¼Œæ›´æ–°å³å¯

```python
### YOUR CODE HERE
loss, grad = f(x)
x -= step * grad

### END YOUR CODE
```

### é—®é¢˜(c) (4åˆ†)

è‡³æ­¤æ‰€æœ‰çš„ä»£ç éƒ½å†™å®Œäº†ï¼Œæ¥ä¸‹æ¥æ˜¯ä¸‹è½½æ•°æ®é›†ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨Stanform Sentiment Treebank(SST)æ•°æ®é›†ï¼Œå®ƒå¯ä»¥ç”¨åœ¨ç®€å•çš„è¯­ä¹‰åˆ†æä»»åŠ¡ä¸­å»ã€‚é€šè¿‡è¿è¡Œ sh get_datasets.sh å¯ä»¥è·å¾—è¯¥æ•°æ®é›†ï¼Œä¸‹è½½å®Œæˆåè¿è¡Œ python run.py å³å¯ã€‚

æ³¨æ„ï¼šè®­ç»ƒçš„æ—¶é—´å–å†³äºä»£ç æ˜¯å¦é«˜æ•ˆï¼ˆå³ä¾¿æ˜¯é«˜æ•ˆçš„å®ç°ï¼Œä¹Ÿè¦è·‘æ¥è¿‘ä¸€ä¸ªå°æ—¶ï¼‰

ç»è¿‡40,000æ¬¡è¿­ä»£åï¼Œæœ€ç»ˆç»“æœä¼šä¿å­˜åˆ° word_vectors.png é‡Œã€‚

**ç­”ï¼š**

<img src="http://ww1.sinaimg.cn/large/0060yMmAly1gsz2rwoc9sj30hs0dc3zy.jpg" referrerpolicy="no-referrer" />

- åœ¨ä¸Šå›¾ä¸­å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œmale->famale å’Œ king -> queen è¿™ä¸¤æ¡å‘é‡æ˜¯å¹³è¡Œçš„
- (women, famale)ï¼Œ(enjoyable,annoying) è¿™äº›å«ä¹‰æ¥è¿‘çš„è¯è·ç¦»å¾ˆè¿‘